{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T20:08:09.445523600Z",
     "start_time": "2026-01-15T20:05:45.313851800Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\76949\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\76949\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从本地路径加载模型: C:\\Users\\76949\\.cache\\huggingface\\hub\\models--microsoft--llava-med-v1.5-mistral-7b\\snapshots\\91bb16c122001ddc9cf1fd36ce1dae09448943a2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava_mistral to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 已设置 tokenizer.model_max_length = 4096\n",
      "✓ Processor 已加载并配置完成\n",
      "使用设备: cuda\n",
      "检测到 GPU 显存: 15.9 GB\n",
      "⚠️ 检测到16GB显存，将先尝试不使用量化（性能更好）\n",
      "   如果出现OOM错误，请重新运行并设置 use_quantization = True\n",
      "\n",
      "尝试不使用量化加载模型（性能更好）...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 75.91it/s]\n",
      "Some weights of LlavaForConditionalGeneration were not initialized from the model checkpoint at C:\\Users\\76949\\.cache\\huggingface\\hub\\models--microsoft--llava-med-v1.5-mistral-7b\\snapshots\\91bb16c122001ddc9cf1fd36ce1dae09448943a2 and are newly initialized: ['model.language_model.embed_tokens.weight', 'model.language_model.layers.0.input_layernorm.weight', 'model.language_model.layers.0.mlp.down_proj.weight', 'model.language_model.layers.0.mlp.gate_proj.weight', 'model.language_model.layers.0.mlp.up_proj.weight', 'model.language_model.layers.0.post_attention_layernorm.weight', 'model.language_model.layers.0.self_attn.k_proj.weight', 'model.language_model.layers.0.self_attn.o_proj.weight', 'model.language_model.layers.0.self_attn.q_proj.weight', 'model.language_model.layers.0.self_attn.v_proj.weight', 'model.language_model.layers.1.input_layernorm.weight', 'model.language_model.layers.1.mlp.down_proj.weight', 'model.language_model.layers.1.mlp.gate_proj.weight', 'model.language_model.layers.1.mlp.up_proj.weight', 'model.language_model.layers.1.post_attention_layernorm.weight', 'model.language_model.layers.1.self_attn.k_proj.weight', 'model.language_model.layers.1.self_attn.o_proj.weight', 'model.language_model.layers.1.self_attn.q_proj.weight', 'model.language_model.layers.1.self_attn.v_proj.weight', 'model.language_model.layers.10.input_layernorm.weight', 'model.language_model.layers.10.mlp.down_proj.weight', 'model.language_model.layers.10.mlp.gate_proj.weight', 'model.language_model.layers.10.mlp.up_proj.weight', 'model.language_model.layers.10.post_attention_layernorm.weight', 'model.language_model.layers.10.self_attn.k_proj.weight', 'model.language_model.layers.10.self_attn.o_proj.weight', 'model.language_model.layers.10.self_attn.q_proj.weight', 'model.language_model.layers.10.self_attn.v_proj.weight', 'model.language_model.layers.11.input_layernorm.weight', 'model.language_model.layers.11.mlp.down_proj.weight', 'model.language_model.layers.11.mlp.gate_proj.weight', 'model.language_model.layers.11.mlp.up_proj.weight', 'model.language_model.layers.11.post_attention_layernorm.weight', 'model.language_model.layers.11.self_attn.k_proj.weight', 'model.language_model.layers.11.self_attn.o_proj.weight', 'model.language_model.layers.11.self_attn.q_proj.weight', 'model.language_model.layers.11.self_attn.v_proj.weight', 'model.language_model.layers.12.input_layernorm.weight', 'model.language_model.layers.12.mlp.down_proj.weight', 'model.language_model.layers.12.mlp.gate_proj.weight', 'model.language_model.layers.12.mlp.up_proj.weight', 'model.language_model.layers.12.post_attention_layernorm.weight', 'model.language_model.layers.12.self_attn.k_proj.weight', 'model.language_model.layers.12.self_attn.o_proj.weight', 'model.language_model.layers.12.self_attn.q_proj.weight', 'model.language_model.layers.12.self_attn.v_proj.weight', 'model.language_model.layers.13.input_layernorm.weight', 'model.language_model.layers.13.mlp.down_proj.weight', 'model.language_model.layers.13.mlp.gate_proj.weight', 'model.language_model.layers.13.mlp.up_proj.weight', 'model.language_model.layers.13.post_attention_layernorm.weight', 'model.language_model.layers.13.self_attn.k_proj.weight', 'model.language_model.layers.13.self_attn.o_proj.weight', 'model.language_model.layers.13.self_attn.q_proj.weight', 'model.language_model.layers.13.self_attn.v_proj.weight', 'model.language_model.layers.14.input_layernorm.weight', 'model.language_model.layers.14.mlp.down_proj.weight', 'model.language_model.layers.14.mlp.gate_proj.weight', 'model.language_model.layers.14.mlp.up_proj.weight', 'model.language_model.layers.14.post_attention_layernorm.weight', 'model.language_model.layers.14.self_attn.k_proj.weight', 'model.language_model.layers.14.self_attn.o_proj.weight', 'model.language_model.layers.14.self_attn.q_proj.weight', 'model.language_model.layers.14.self_attn.v_proj.weight', 'model.language_model.layers.15.input_layernorm.weight', 'model.language_model.layers.15.mlp.down_proj.weight', 'model.language_model.layers.15.mlp.gate_proj.weight', 'model.language_model.layers.15.mlp.up_proj.weight', 'model.language_model.layers.15.post_attention_layernorm.weight', 'model.language_model.layers.15.self_attn.k_proj.weight', 'model.language_model.layers.15.self_attn.o_proj.weight', 'model.language_model.layers.15.self_attn.q_proj.weight', 'model.language_model.layers.15.self_attn.v_proj.weight', 'model.language_model.layers.16.input_layernorm.weight', 'model.language_model.layers.16.mlp.down_proj.weight', 'model.language_model.layers.16.mlp.gate_proj.weight', 'model.language_model.layers.16.mlp.up_proj.weight', 'model.language_model.layers.16.post_attention_layernorm.weight', 'model.language_model.layers.16.self_attn.k_proj.weight', 'model.language_model.layers.16.self_attn.o_proj.weight', 'model.language_model.layers.16.self_attn.q_proj.weight', 'model.language_model.layers.16.self_attn.v_proj.weight', 'model.language_model.layers.17.input_layernorm.weight', 'model.language_model.layers.17.mlp.down_proj.weight', 'model.language_model.layers.17.mlp.gate_proj.weight', 'model.language_model.layers.17.mlp.up_proj.weight', 'model.language_model.layers.17.post_attention_layernorm.weight', 'model.language_model.layers.17.self_attn.k_proj.weight', 'model.language_model.layers.17.self_attn.o_proj.weight', 'model.language_model.layers.17.self_attn.q_proj.weight', 'model.language_model.layers.17.self_attn.v_proj.weight', 'model.language_model.layers.18.input_layernorm.weight', 'model.language_model.layers.18.mlp.down_proj.weight', 'model.language_model.layers.18.mlp.gate_proj.weight', 'model.language_model.layers.18.mlp.up_proj.weight', 'model.language_model.layers.18.post_attention_layernorm.weight', 'model.language_model.layers.18.self_attn.k_proj.weight', 'model.language_model.layers.18.self_attn.o_proj.weight', 'model.language_model.layers.18.self_attn.q_proj.weight', 'model.language_model.layers.18.self_attn.v_proj.weight', 'model.language_model.layers.19.input_layernorm.weight', 'model.language_model.layers.19.mlp.down_proj.weight', 'model.language_model.layers.19.mlp.gate_proj.weight', 'model.language_model.layers.19.mlp.up_proj.weight', 'model.language_model.layers.19.post_attention_layernorm.weight', 'model.language_model.layers.19.self_attn.k_proj.weight', 'model.language_model.layers.19.self_attn.o_proj.weight', 'model.language_model.layers.19.self_attn.q_proj.weight', 'model.language_model.layers.19.self_attn.v_proj.weight', 'model.language_model.layers.2.input_layernorm.weight', 'model.language_model.layers.2.mlp.down_proj.weight', 'model.language_model.layers.2.mlp.gate_proj.weight', 'model.language_model.layers.2.mlp.up_proj.weight', 'model.language_model.layers.2.post_attention_layernorm.weight', 'model.language_model.layers.2.self_attn.k_proj.weight', 'model.language_model.layers.2.self_attn.o_proj.weight', 'model.language_model.layers.2.self_attn.q_proj.weight', 'model.language_model.layers.2.self_attn.v_proj.weight', 'model.language_model.layers.20.input_layernorm.weight', 'model.language_model.layers.20.mlp.down_proj.weight', 'model.language_model.layers.20.mlp.gate_proj.weight', 'model.language_model.layers.20.mlp.up_proj.weight', 'model.language_model.layers.20.post_attention_layernorm.weight', 'model.language_model.layers.20.self_attn.k_proj.weight', 'model.language_model.layers.20.self_attn.o_proj.weight', 'model.language_model.layers.20.self_attn.q_proj.weight', 'model.language_model.layers.20.self_attn.v_proj.weight', 'model.language_model.layers.21.input_layernorm.weight', 'model.language_model.layers.21.mlp.down_proj.weight', 'model.language_model.layers.21.mlp.gate_proj.weight', 'model.language_model.layers.21.mlp.up_proj.weight', 'model.language_model.layers.21.post_attention_layernorm.weight', 'model.language_model.layers.21.self_attn.k_proj.weight', 'model.language_model.layers.21.self_attn.o_proj.weight', 'model.language_model.layers.21.self_attn.q_proj.weight', 'model.language_model.layers.21.self_attn.v_proj.weight', 'model.language_model.layers.22.input_layernorm.weight', 'model.language_model.layers.22.mlp.down_proj.weight', 'model.language_model.layers.22.mlp.gate_proj.weight', 'model.language_model.layers.22.mlp.up_proj.weight', 'model.language_model.layers.22.post_attention_layernorm.weight', 'model.language_model.layers.22.self_attn.k_proj.weight', 'model.language_model.layers.22.self_attn.o_proj.weight', 'model.language_model.layers.22.self_attn.q_proj.weight', 'model.language_model.layers.22.self_attn.v_proj.weight', 'model.language_model.layers.23.input_layernorm.weight', 'model.language_model.layers.23.mlp.down_proj.weight', 'model.language_model.layers.23.mlp.gate_proj.weight', 'model.language_model.layers.23.mlp.up_proj.weight', 'model.language_model.layers.23.post_attention_layernorm.weight', 'model.language_model.layers.23.self_attn.k_proj.weight', 'model.language_model.layers.23.self_attn.o_proj.weight', 'model.language_model.layers.23.self_attn.q_proj.weight', 'model.language_model.layers.23.self_attn.v_proj.weight', 'model.language_model.layers.24.input_layernorm.weight', 'model.language_model.layers.24.mlp.down_proj.weight', 'model.language_model.layers.24.mlp.gate_proj.weight', 'model.language_model.layers.24.mlp.up_proj.weight', 'model.language_model.layers.24.post_attention_layernorm.weight', 'model.language_model.layers.24.self_attn.k_proj.weight', 'model.language_model.layers.24.self_attn.o_proj.weight', 'model.language_model.layers.24.self_attn.q_proj.weight', 'model.language_model.layers.24.self_attn.v_proj.weight', 'model.language_model.layers.25.input_layernorm.weight', 'model.language_model.layers.25.mlp.down_proj.weight', 'model.language_model.layers.25.mlp.gate_proj.weight', 'model.language_model.layers.25.mlp.up_proj.weight', 'model.language_model.layers.25.post_attention_layernorm.weight', 'model.language_model.layers.25.self_attn.k_proj.weight', 'model.language_model.layers.25.self_attn.o_proj.weight', 'model.language_model.layers.25.self_attn.q_proj.weight', 'model.language_model.layers.25.self_attn.v_proj.weight', 'model.language_model.layers.26.input_layernorm.weight', 'model.language_model.layers.26.mlp.down_proj.weight', 'model.language_model.layers.26.mlp.gate_proj.weight', 'model.language_model.layers.26.mlp.up_proj.weight', 'model.language_model.layers.26.post_attention_layernorm.weight', 'model.language_model.layers.26.self_attn.k_proj.weight', 'model.language_model.layers.26.self_attn.o_proj.weight', 'model.language_model.layers.26.self_attn.q_proj.weight', 'model.language_model.layers.26.self_attn.v_proj.weight', 'model.language_model.layers.27.input_layernorm.weight', 'model.language_model.layers.27.mlp.down_proj.weight', 'model.language_model.layers.27.mlp.gate_proj.weight', 'model.language_model.layers.27.mlp.up_proj.weight', 'model.language_model.layers.27.post_attention_layernorm.weight', 'model.language_model.layers.27.self_attn.k_proj.weight', 'model.language_model.layers.27.self_attn.o_proj.weight', 'model.language_model.layers.27.self_attn.q_proj.weight', 'model.language_model.layers.27.self_attn.v_proj.weight', 'model.language_model.layers.28.input_layernorm.weight', 'model.language_model.layers.28.mlp.down_proj.weight', 'model.language_model.layers.28.mlp.gate_proj.weight', 'model.language_model.layers.28.mlp.up_proj.weight', 'model.language_model.layers.28.post_attention_layernorm.weight', 'model.language_model.layers.28.self_attn.k_proj.weight', 'model.language_model.layers.28.self_attn.o_proj.weight', 'model.language_model.layers.28.self_attn.q_proj.weight', 'model.language_model.layers.28.self_attn.v_proj.weight', 'model.language_model.layers.29.input_layernorm.weight', 'model.language_model.layers.29.mlp.down_proj.weight', 'model.language_model.layers.29.mlp.gate_proj.weight', 'model.language_model.layers.29.mlp.up_proj.weight', 'model.language_model.layers.29.post_attention_layernorm.weight', 'model.language_model.layers.29.self_attn.k_proj.weight', 'model.language_model.layers.29.self_attn.o_proj.weight', 'model.language_model.layers.29.self_attn.q_proj.weight', 'model.language_model.layers.29.self_attn.v_proj.weight', 'model.language_model.layers.3.input_layernorm.weight', 'model.language_model.layers.3.mlp.down_proj.weight', 'model.language_model.layers.3.mlp.gate_proj.weight', 'model.language_model.layers.3.mlp.up_proj.weight', 'model.language_model.layers.3.post_attention_layernorm.weight', 'model.language_model.layers.3.self_attn.k_proj.weight', 'model.language_model.layers.3.self_attn.o_proj.weight', 'model.language_model.layers.3.self_attn.q_proj.weight', 'model.language_model.layers.3.self_attn.v_proj.weight', 'model.language_model.layers.30.input_layernorm.weight', 'model.language_model.layers.30.mlp.down_proj.weight', 'model.language_model.layers.30.mlp.gate_proj.weight', 'model.language_model.layers.30.mlp.up_proj.weight', 'model.language_model.layers.30.post_attention_layernorm.weight', 'model.language_model.layers.30.self_attn.k_proj.weight', 'model.language_model.layers.30.self_attn.o_proj.weight', 'model.language_model.layers.30.self_attn.q_proj.weight', 'model.language_model.layers.30.self_attn.v_proj.weight', 'model.language_model.layers.31.input_layernorm.weight', 'model.language_model.layers.31.mlp.down_proj.weight', 'model.language_model.layers.31.mlp.gate_proj.weight', 'model.language_model.layers.31.mlp.up_proj.weight', 'model.language_model.layers.31.post_attention_layernorm.weight', 'model.language_model.layers.31.self_attn.k_proj.weight', 'model.language_model.layers.31.self_attn.o_proj.weight', 'model.language_model.layers.31.self_attn.q_proj.weight', 'model.language_model.layers.31.self_attn.v_proj.weight', 'model.language_model.layers.4.input_layernorm.weight', 'model.language_model.layers.4.mlp.down_proj.weight', 'model.language_model.layers.4.mlp.gate_proj.weight', 'model.language_model.layers.4.mlp.up_proj.weight', 'model.language_model.layers.4.post_attention_layernorm.weight', 'model.language_model.layers.4.self_attn.k_proj.weight', 'model.language_model.layers.4.self_attn.o_proj.weight', 'model.language_model.layers.4.self_attn.q_proj.weight', 'model.language_model.layers.4.self_attn.v_proj.weight', 'model.language_model.layers.5.input_layernorm.weight', 'model.language_model.layers.5.mlp.down_proj.weight', 'model.language_model.layers.5.mlp.gate_proj.weight', 'model.language_model.layers.5.mlp.up_proj.weight', 'model.language_model.layers.5.post_attention_layernorm.weight', 'model.language_model.layers.5.self_attn.k_proj.weight', 'model.language_model.layers.5.self_attn.o_proj.weight', 'model.language_model.layers.5.self_attn.q_proj.weight', 'model.language_model.layers.5.self_attn.v_proj.weight', 'model.language_model.layers.6.input_layernorm.weight', 'model.language_model.layers.6.mlp.down_proj.weight', 'model.language_model.layers.6.mlp.gate_proj.weight', 'model.language_model.layers.6.mlp.up_proj.weight', 'model.language_model.layers.6.post_attention_layernorm.weight', 'model.language_model.layers.6.self_attn.k_proj.weight', 'model.language_model.layers.6.self_attn.o_proj.weight', 'model.language_model.layers.6.self_attn.q_proj.weight', 'model.language_model.layers.6.self_attn.v_proj.weight', 'model.language_model.layers.7.input_layernorm.weight', 'model.language_model.layers.7.mlp.down_proj.weight', 'model.language_model.layers.7.mlp.gate_proj.weight', 'model.language_model.layers.7.mlp.up_proj.weight', 'model.language_model.layers.7.post_attention_layernorm.weight', 'model.language_model.layers.7.self_attn.k_proj.weight', 'model.language_model.layers.7.self_attn.o_proj.weight', 'model.language_model.layers.7.self_attn.q_proj.weight', 'model.language_model.layers.7.self_attn.v_proj.weight', 'model.language_model.layers.8.input_layernorm.weight', 'model.language_model.layers.8.mlp.down_proj.weight', 'model.language_model.layers.8.mlp.gate_proj.weight', 'model.language_model.layers.8.mlp.up_proj.weight', 'model.language_model.layers.8.post_attention_layernorm.weight', 'model.language_model.layers.8.self_attn.k_proj.weight', 'model.language_model.layers.8.self_attn.o_proj.weight', 'model.language_model.layers.8.self_attn.q_proj.weight', 'model.language_model.layers.8.self_attn.v_proj.weight', 'model.language_model.layers.9.input_layernorm.weight', 'model.language_model.layers.9.mlp.down_proj.weight', 'model.language_model.layers.9.mlp.gate_proj.weight', 'model.language_model.layers.9.mlp.up_proj.weight', 'model.language_model.layers.9.post_attention_layernorm.weight', 'model.language_model.layers.9.self_attn.k_proj.weight', 'model.language_model.layers.9.self_attn.o_proj.weight', 'model.language_model.layers.9.self_attn.q_proj.weight', 'model.language_model.layers.9.self_attn.v_proj.weight', 'model.language_model.norm.weight', 'model.multi_modal_projector.linear_1.bias', 'model.multi_modal_projector.linear_1.weight', 'model.multi_modal_projector.linear_2.bias', 'model.multi_modal_projector.linear_2.weight', 'model.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_model.pre_layrnorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 模型已从本地路径加载完成（无量化，性能最佳）\n",
      "✓ 模型配置中的 image_token_id: 32000\n",
      "\n",
      "✓ LLaVA-Med 模型与 Processor 已加载。\n"
     ]
    }
   ],
   "source": [
    "# ==== 使用 LLaVA-Med 进行微调 ====\n",
    "\n",
    "from transformers import (\n",
    "    LlavaForConditionalGeneration,\n",
    "    LlavaProcessor,\n",
    "    AutoTokenizer,\n",
    "    CLIPImageProcessor,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import torch\n",
    "\n",
    "# 从本地路径加载模型\n",
    "model_local_path = r\"C:\\Users\\76949\\.cache\\huggingface\\hub\\models--microsoft--llava-med-v1.5-mistral-7b\\snapshots\\91bb16c122001ddc9cf1fd36ce1dae09448943a2\"\n",
    "\n",
    "print(f\"从本地路径加载模型: {model_local_path}\")\n",
    "\n",
    "# 加载 processor\n",
    "# 创建 image_processor 时设置正确的图像尺寸（336x336）\n",
    "image_processor = CLIPImageProcessor.from_pretrained(\n",
    "    \"openai/clip-vit-large-patch14\",\n",
    "    size={\"height\": 336, \"width\": 336},  # LLaVA-Med 模型期望的尺寸\n",
    "    crop_size={\"height\": 336, \"width\": 336}\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_local_path, use_fast=False)\n",
    "\n",
    "processor = LlavaProcessor(\n",
    "    tokenizer=tokenizer,\n",
    "    image_processor=image_processor\n",
    ")\n",
    "\n",
    "# 补全 LLaVA-Med 必要属性\n",
    "processor.patch_size = 14\n",
    "processor.num_additional_image_tokens = 0\n",
    "processor.vision_feature_select_strategy = \"default\"\n",
    "\n",
    "# 确保 image_processor 的尺寸设置正确\n",
    "processor.image_processor.size = {\"height\": 336, \"width\": 336}\n",
    "processor.image_processor.crop_size = {\"height\": 336, \"width\": 336}\n",
    "\n",
    "# 设置 tokenizer 的 model_max_length 足够大，避免截断图像 token\n",
    "# LLaVA-Med 的图像 token 数量约为 576，加上文本需要更大的长度\n",
    "if hasattr(processor.tokenizer, 'model_max_length'):\n",
    "    # 如果 model_max_length 太小，设置为一个更大的值\n",
    "    if processor.tokenizer.model_max_length < 4096:\n",
    "        processor.tokenizer.model_max_length = 4096\n",
    "        print(f\"✓ 已设置 tokenizer.model_max_length = {processor.tokenizer.model_max_length}\")\n",
    "\n",
    "print(\"✓ Processor 已加载并配置完成\")\n",
    "\n",
    "# 设置 device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 检测显存大小（16GB显存的智能策略）\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"检测到 GPU 显存: {gpu_memory_gb:.1f} GB\")\n",
    "\n",
    "    # 16GB显存的策略：先尝试不使用量化，如果OOM再使用量化\n",
    "    if 15.0 <= gpu_memory_gb <= 17.0:  # 16GB左右\n",
    "        print(\"⚠️ 检测到16GB显存，将先尝试不使用量化（性能更好）\")\n",
    "        print(\"   如果出现OOM错误，请重新运行并设置 use_quantization = True\")\n",
    "        use_quantization = False  # 16GB可以尝试不使用量化\n",
    "    elif gpu_memory_gb < 15.0:\n",
    "        print(\"⚠️ 显存 < 15GB，建议使用量化\")\n",
    "        use_quantization = True\n",
    "    else:\n",
    "        print(\"✓ 显存充足，不使用量化\")\n",
    "        use_quantization = False\n",
    "else:\n",
    "    use_quantization = False\n",
    "    print(\"CPU模式，不使用量化\")\n",
    "\n",
    "# 如果显存正好16GB，可以手动设置策略：\n",
    "# use_quantization = False  # 尝试不使用量化（性能更好，但可能OOM）\n",
    "# use_quantization = True   # 使用量化（更安全，但性能略低）\n",
    "\n",
    "# 尝试加载模型\n",
    "model_loaded = False\n",
    "\n",
    "# 策略1：先尝试不使用量化（16GB显存可能可以）\n",
    "if not use_quantization and torch.cuda.is_available():\n",
    "    try:\n",
    "        print(\"\\n尝试不使用量化加载模型（性能更好）...\")\n",
    "        model = LlavaForConditionalGeneration.from_pretrained(\n",
    "            model_local_path,\n",
    "            dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        print(\"✓ 模型已从本地路径加载完成（无量化，性能最佳）\")\n",
    "        model_loaded = True\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower() or \"OOM\" in str(e):\n",
    "            print(f\"\\n⚠️ 显存不足（OOM），自动切换到4-bit量化...\")\n",
    "            torch.cuda.empty_cache()  # 清除缓存\n",
    "            use_quantization = True\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# 策略2：使用4-bit量化（如果策略1失败或显存不足）\n",
    "if not model_loaded and use_quantization and torch.cuda.is_available():\n",
    "    try:\n",
    "        print(\"\\n使用 4-bit 量化加载模型（节省显存）...\")\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "        model = LlavaForConditionalGeneration.from_pretrained(\n",
    "            model_local_path,\n",
    "            dtype=torch.float16,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        print(\"✓ 模型已从本地路径加载完成（4-bit 量化，显存占用约6-8GB）\")\n",
    "        model_loaded = True\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 量化加载也失败: {e}\")\n",
    "        raise e\n",
    "\n",
    "# CPU模式\n",
    "if not model_loaded:\n",
    "    print(\"\\n使用 CPU 模式加载模型...\")\n",
    "    model = LlavaForConditionalGeneration.from_pretrained(\n",
    "        model_local_path,\n",
    "        dtype=torch.float32,\n",
    "        device_map=None,\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    print(\"✓ 模型已从本地路径加载完成（CPU模式）\")\n",
    "\n",
    "# 检查并设置图像 token ID（在模型加载完成后）\n",
    "# LLaVA 模型需要知道图像 token ID 来识别图像 token\n",
    "image_token_id = None\n",
    "if hasattr(model, 'config'):\n",
    "    if hasattr(model.config, 'image_token_id'):\n",
    "        image_token_id = model.config.image_token_id\n",
    "        print(f\"✓ 模型配置中的 image_token_id: {image_token_id}\")\n",
    "    else:\n",
    "        # 尝试从 tokenizer 获取\n",
    "        try:\n",
    "            # 检查 tokenizer 是否有 image_token_id\n",
    "            if hasattr(processor.tokenizer, 'convert_tokens_to_ids'):\n",
    "                test_id = processor.tokenizer.convert_tokens_to_ids('<image>')\n",
    "                if test_id != processor.tokenizer.unk_token_id:\n",
    "                    image_token_id = test_id\n",
    "                    print(f\"✓ 从 tokenizer 获取 <image> token ID: {image_token_id}\")\n",
    "                else:\n",
    "                    # 尝试常见的图像 token ID\n",
    "                    # LLaVA 通常使用词汇表末尾的 token\n",
    "                    vocab_size = getattr(processor.tokenizer, 'vocab_size', None)\n",
    "                    if vocab_size:\n",
    "                        # 使用词汇表末尾的 token（通常是预留的特殊 token）\n",
    "                        image_token_id = vocab_size - 1\n",
    "                        print(f\"⚠️ <image> token 不在词汇表中，使用 vocab_size - 1 = {image_token_id}\")\n",
    "                    else:\n",
    "                        image_token_id = 32000\n",
    "                        print(f\"⚠️ 无法获取词汇表大小，使用默认值 32000\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 无法获取图像 token ID: {e}\")\n",
    "            # 尝试使用词汇表末尾的 token\n",
    "            vocab_size = getattr(processor.tokenizer, 'vocab_size', None)\n",
    "            if vocab_size:\n",
    "                image_token_id = vocab_size - 1\n",
    "            else:\n",
    "                image_token_id = 32000\n",
    "\n",
    "# 验证并修正图像 token ID（确保它在有效范围内）\n",
    "vocab_size = getattr(processor.tokenizer, 'vocab_size', None)\n",
    "if vocab_size and image_token_id is not None and image_token_id >= vocab_size:\n",
    "    image_token_id = vocab_size - 1\n",
    "    print(f\"⚠️ 图像 token ID 超出范围，已修正为 vocab_size - 1 = {image_token_id}\")\n",
    "\n",
    "# 如果找到了 image_token_id，确保模型配置中有它\n",
    "if image_token_id is not None and hasattr(model, 'config'):\n",
    "    if not hasattr(model.config, 'image_token_id'):\n",
    "        model.config.image_token_id = image_token_id\n",
    "        print(f\"✓ 已设置 model.config.image_token_id = {image_token_id}\")\n",
    "\n",
    "print(\"\\n✓ LLaVA-Med 模型与 Processor 已加载。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d42d798",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T20:08:09.467360400Z",
     "start_time": "2026-01-15T20:08:09.446523600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 工具函数已定义\n"
     ]
    }
   ],
   "source": [
    "# ==== 步骤1: 导入必要的库和定义工具函数 ====\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import Tuple, List\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# 数据集路径\n",
    "DATA_JSON_PATH = r\"data\\archive (1)\\VQA_RAD Dataset Public.json\"\n",
    "DATA_IMAGE_DIR = r\"data\\archive (1)\\VQA_RAD Image Folder\"\n",
    "\n",
    "def normalize_answer(ans: str) -> str:\n",
    "    \"\"\"简单归一化答案，用于区分close(是/否)和open类型。\"\"\"\n",
    "    if ans is None:\n",
    "        return \"\"\n",
    "    ans = str(ans).lower().strip()\n",
    "    ans_clean = ans.rstrip('.,!?;:').strip()\n",
    "\n",
    "    # 先规范 yes/no\n",
    "    if ans_clean == \"yes\":\n",
    "        return \"yes\"\n",
    "    if ans_clean == \"no\":\n",
    "        return \"no\"\n",
    "\n",
    "    if ans_clean.startswith(\"yes\"):\n",
    "        if len(ans_clean) == 3 or ans_clean[3] in [\" \", \",\", \".\", \"!\", \"?\", \";\", \":\"]:\n",
    "            return \"yes\"\n",
    "    if ans_clean.startswith(\"no\"):\n",
    "        if len(ans_clean) == 2 or ans_clean[2] in [\" \", \",\", \".\", \"!\", \"?\", \";\", \":\"]:\n",
    "            return \"no\"\n",
    "\n",
    "    # 针对 open 问题的一些简单同义合并\n",
    "    synonym_map = {\n",
    "        \"right side\": \"right\",\n",
    "        \"left side\": \"left\",\n",
    "        \"rt\": \"right\",\n",
    "        \"lt\": \"left\",\n",
    "        \"xray\": \"x-ray\",\n",
    "        \"x ray\": \"x-ray\",\n",
    "        \"ct scan\": \"ct\",\n",
    "    }\n",
    "    if ans_clean in synonym_map:\n",
    "        return synonym_map[ans_clean]\n",
    "    \n",
    "    ans_clean = ans_clean.replace(\"xray\", \"x-ray\").replace(\"x ray\", \"x-ray\")\n",
    "    return ans_clean\n",
    "\n",
    "print(\"✓ 工具函数已定义\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14631a25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T20:08:09.479868300Z",
     "start_time": "2026-01-15T20:08:09.467360400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LLaVA 数据集类已定义\n"
     ]
    }
   ],
   "source": [
    "# ==== 步骤2: 创建符合 LLaVA 标准的数据集类 ====\n",
    "# LLaVA 需要原始 PIL 图像，不使用 torchvision 的 transform\n",
    "# 图像处理由 processor 的 image_processor 完成\n",
    "\n",
    "class VQARADLLaVADataset(Dataset):\n",
    "    \"\"\"符合 LLaVA 标准的数据集类\n",
    "    \n",
    "    返回原始 PIL 图像，让 processor 来处理图像预处理\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, json_path: str, image_dir: str, indices: List[int] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            json_path: JSON 数据文件路径\n",
    "            image_dir: 图像目录路径\n",
    "            indices: 可选，指定使用的数据索引列表。如果为 None，使用全部数据\n",
    "        \"\"\"\n",
    "        assert os.path.exists(json_path), f\"JSON 文件不存在: {json_path}\"\n",
    "        assert os.path.exists(image_dir), f\"图像目录不存在: {image_dir}\"\n",
    "        \n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.data = json.load(f)\n",
    "        \n",
    "        self.image_dir = image_dir\n",
    "        self.indices = indices if indices is not None else list(range(len(self.data)))\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def _get_image_path(self, item) -> str:\n",
    "        \"\"\"获取图像路径\"\"\"\n",
    "        image_key = \"image\" if \"image\" in item else \"image_name\"\n",
    "        return os.path.join(self.image_dir, item[image_key])\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[Image.Image, str, str]:\n",
    "        \"\"\"\n",
    "        返回: (PIL Image, question, answer)\n",
    "        \n",
    "        - 返回原始 PIL 图像，不使用 transform\n",
    "        - question: 问题文本\n",
    "        - answer: 答案文本（原始答案，用于训练）\n",
    "        \"\"\"\n",
    "        real_idx = self.indices[idx]\n",
    "        item = self.data[real_idx]\n",
    "        \n",
    "        # 读取原始图像（PIL Image，不做任何 transform）\n",
    "        image_path = self._get_image_path(item)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        question = item.get(\"question\", \"\")\n",
    "        answer = item.get(\"answer\", \"\")\n",
    "        \n",
    "        return image, question, answer\n",
    "\n",
    "print(\"✓ LLaVA 数据集类已定义\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d77dfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T20:08:09.532880700Z",
     "start_time": "2026-01-15T20:08:09.480868300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集总样本数: 2248\n",
      "Close (是/否) 样本数: 1193\n",
      "Open  (开放式) 样本数: 1055\n",
      "\n",
      "Close (yes/no) 数据集划分:\n",
      "  训练集: 954 个样本 (80.0%)\n",
      "  测试集: 239 个样本 (20.0%)\n",
      "\n",
      "Open 数据集划分:\n",
      "  训练集: 844 个样本 (80.0%)\n",
      "  测试集: 211 个样本 (20.0%)\n",
      "\n",
      "✓ LLaVA 数据集实例已创建\n"
     ]
    }
   ],
   "source": [
    "# ==== 步骤3: 划分数据集并创建 LLaVA 数据集实例 ====\n",
    "\n",
    "# 设置随机种子\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# 加载原始数据以进行划分\n",
    "with open(DATA_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# 根据答案类型划分 close 和 open\n",
    "close_indices = []\n",
    "open_indices = []\n",
    "\n",
    "for idx, item in enumerate(raw_data):\n",
    "    answer_raw = item.get(\"answer\", \"\")\n",
    "    ans_norm = normalize_answer(answer_raw)\n",
    "    if ans_norm in [\"yes\", \"no\"]:\n",
    "        close_indices.append(idx)\n",
    "    else:\n",
    "        open_indices.append(idx)\n",
    "\n",
    "print(f\"数据集总样本数: {len(raw_data)}\")\n",
    "print(f\"Close (是/否) 样本数: {len(close_indices)}\")\n",
    "print(f\"Open  (开放式) 样本数: {len(open_indices)}\")\n",
    "\n",
    "# 对 close 和 open 数据集分别进行 8:2 划分\n",
    "close_train_idx, close_test_idx = train_test_split(\n",
    "    close_indices,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "open_train_idx, open_test_idx = train_test_split(\n",
    "    open_indices,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"\\nClose (yes/no) 数据集划分:\")\n",
    "print(f\"  训练集: {len(close_train_idx)} 个样本 ({len(close_train_idx)/len(close_indices)*100:.1f}%)\")\n",
    "print(f\"  测试集: {len(close_test_idx)} 个样本 ({len(close_test_idx)/len(close_indices)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nOpen 数据集划分:\")\n",
    "print(f\"  训练集: {len(open_train_idx)} 个样本 ({len(open_train_idx)/len(open_indices)*100:.1f}%)\")\n",
    "print(f\"  测试集: {len(open_test_idx)} 个样本 ({len(open_test_idx)/len(open_indices)*100:.1f}%)\")\n",
    "\n",
    "# 创建 LLaVA 数据集实例\n",
    "llava_close_train_dataset = VQARADLLaVADataset(DATA_JSON_PATH, DATA_IMAGE_DIR, close_train_idx)\n",
    "llava_close_test_dataset = VQARADLLaVADataset(DATA_JSON_PATH, DATA_IMAGE_DIR, close_test_idx)\n",
    "\n",
    "llava_open_train_dataset = VQARADLLaVADataset(DATA_JSON_PATH, DATA_IMAGE_DIR, open_train_idx)\n",
    "llava_open_test_dataset = VQARADLLaVADataset(DATA_JSON_PATH, DATA_IMAGE_DIR, open_test_idx)\n",
    "\n",
    "print(\"\\n✓ LLaVA 数据集实例已创建\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca52c196",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T20:10:57.580517600Z",
     "start_time": "2026-01-15T20:10:57.562578500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LLaVA collate 函数已定义\n"
     ]
    }
   ],
   "source": [
    "# ==== 步骤4: 实现符合 LLaVA 标准的 collate 函数 ====\n",
    "# 使用 processor 的标准方法来处理数据\n",
    "\n",
    "def get_valid_image_token_id():\n",
    "    \"\"\"获取并验证图像 token ID 是否在有效范围内\"\"\"\n",
    "    image_token_id = None\n",
    "    \n",
    "    # 首先尝试从模型配置获取\n",
    "    if hasattr(model, 'config') and hasattr(model.config, 'image_token_id'):\n",
    "        image_token_id = model.config.image_token_id\n",
    "    \n",
    "    # 获取 tokenizer 的词汇表大小\n",
    "    vocab_size = getattr(processor.tokenizer, 'vocab_size', None)\n",
    "    if vocab_size is None:\n",
    "        try:\n",
    "            if hasattr(processor.tokenizer, 'get_vocab'):\n",
    "                vocab = processor.tokenizer.get_vocab()\n",
    "                vocab_size = len(vocab) if vocab else None\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # 验证并修正图像 token ID\n",
    "    if vocab_size:\n",
    "        if image_token_id is None or image_token_id >= vocab_size:\n",
    "            # 使用词汇表末尾的 token（通常是预留的特殊 token）\n",
    "            image_token_id = vocab_size - 1\n",
    "    else:\n",
    "        # 如果无法获取词汇表大小，使用默认值\n",
    "        if image_token_id is None:\n",
    "            image_token_id = 32000\n",
    "    \n",
    "    return image_token_id\n",
    "\n",
    "def llava_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    LLaVA 标准的 collate 函数\n",
    "    \n",
    "    手动处理图像 token 插入，因为 processor 可能无法正确识别 <image> token\n",
    "    \"\"\"\n",
    "    images, questions, answers = zip(*batch)\n",
    "    \n",
    "    # 获取图像 token ID（使用辅助函数确保有效性）\n",
    "    image_token_id = get_valid_image_token_id()\n",
    "    \n",
    "    # 调试信息（仅第一次打印）\n",
    "    if not hasattr(llava_collate_fn, '_debug_printed'):\n",
    "        vocab_size = getattr(processor.tokenizer, 'vocab_size', 'unknown')\n",
    "        print(f\"调试信息: vocab_size = {vocab_size}, image_token_id = {image_token_id}\")\n",
    "        llava_collate_fn._debug_printed = True\n",
    "    \n",
    "    # 计算每个图像需要多少个 token（336/14 * 336/14 = 576）\n",
    "    patch_size = getattr(processor, 'patch_size', 14)\n",
    "    image_size = 336\n",
    "    num_image_tokens = (image_size // patch_size) ** 2  # 576\n",
    "    \n",
    "    # 构建对话格式：USER: <image>\\n{question}\\nASSISTANT: {answer}\n",
    "    conversations = []\n",
    "    for q, a in zip(questions, answers):\n",
    "        conversations.append(f\"USER: <image>\\n{q}\\nASSISTANT: {a}\")\n",
    "    \n",
    "    # 处理图像\n",
    "    image_inputs = processor.image_processor(\n",
    "        list(images),\n",
    "        return_tensors=\"pt\",\n",
    "        size={\"height\": 336, \"width\": 336},\n",
    "        do_resize=True,\n",
    "        do_rescale=True,\n",
    "        do_normalize=True\n",
    "    )\n",
    "    \n",
    "    # 手动处理文本，插入图像 token\n",
    "    processed_input_ids = []\n",
    "    processed_attention_masks = []\n",
    "    \n",
    "    for conv in conversations:\n",
    "        # 找到 <image> 的位置\n",
    "        image_pos = conv.find('<image>')\n",
    "        if image_pos != -1:\n",
    "            # 分割文本\n",
    "            before_image = conv[:image_pos]\n",
    "            after_image = conv[image_pos + len('<image>'):]\n",
    "            \n",
    "            # 编码文本部分\n",
    "            before_tokens = processor.tokenizer.encode(before_image, add_special_tokens=False)\n",
    "            after_tokens = processor.tokenizer.encode(after_image, add_special_tokens=False)\n",
    "            \n",
    "            # 插入图像 token（576 个）\n",
    "            image_tokens = [image_token_id] * num_image_tokens\n",
    "            \n",
    "            # 组合：before + image_tokens + after\n",
    "            full_tokens = before_tokens + image_tokens + after_tokens\n",
    "        else:\n",
    "            # 如果没有 <image>，直接编码\n",
    "            full_tokens = processor.tokenizer.encode(conv, add_special_tokens=False)\n",
    "        \n",
    "        processed_input_ids.append(full_tokens)\n",
    "    \n",
    "    # Padding 和截断\n",
    "    # 限制最大长度避免 CUDA 超时（图像 token 576 + 文本，总共不超过 1024）\n",
    "    max_length = 1024  # 减小最大长度避免超时\n",
    "    max_len = max(len(ids) for ids in processed_input_ids)\n",
    "    max_len = min(max_len, max_length)\n",
    "    \n",
    "    padded_input_ids = []\n",
    "    padded_attention_masks = []\n",
    "    \n",
    "    for ids in processed_input_ids:\n",
    "        if len(ids) > max_len:\n",
    "            ids = ids[:max_len]\n",
    "        \n",
    "        attention_mask = [1] * len(ids) + [0] * (max_len - len(ids))\n",
    "        ids = ids + [processor.tokenizer.pad_token_id] * (max_len - len(ids))\n",
    "        \n",
    "        padded_input_ids.append(ids)\n",
    "        padded_attention_masks.append(attention_mask)\n",
    "    \n",
    "    # 转换为 tensor\n",
    "    inputs = {\n",
    "        \"input_ids\": torch.tensor(padded_input_ids, dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor(padded_attention_masks, dtype=torch.long),\n",
    "        \"pixel_values\": image_inputs[\"pixel_values\"],\n",
    "    }\n",
    "    \n",
    "    # 构建 labels（只对答案部分计算 loss）\n",
    "    labels = inputs[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    # 只对 ASSISTANT 的答案部分计算 loss\n",
    "    for i, conv in enumerate(conversations):\n",
    "        # 找到 ASSISTANT: 的位置\n",
    "        assistant_pos = conv.find(\"ASSISTANT:\")\n",
    "        if assistant_pos != -1:\n",
    "            # 获取 ASSISTANT: 之前的部分（包括 ASSISTANT:）\n",
    "            prompt_before_answer = conv[:assistant_pos + len(\"ASSISTANT:\")]\n",
    "            \n",
    "            # 手动编码 prompt（需要考虑图像 token）\n",
    "            image_pos = prompt_before_answer.find('<image>')\n",
    "            if image_pos != -1:\n",
    "                before_image = prompt_before_answer[:image_pos]\n",
    "                after_image = prompt_before_answer[image_pos + len('<image>'):]\n",
    "                before_tokens = processor.tokenizer.encode(before_image, add_special_tokens=False)\n",
    "                after_tokens = processor.tokenizer.encode(after_image, add_special_tokens=False)\n",
    "                prompt_tokens = before_tokens + [image_token_id] * num_image_tokens + after_tokens\n",
    "            else:\n",
    "                prompt_tokens = processor.tokenizer.encode(prompt_before_answer, add_special_tokens=False)\n",
    "            \n",
    "            # 将 prompt 部分的 label 设为 -100\n",
    "            for j in range(min(len(prompt_tokens), labels.shape[1])):\n",
    "                labels[i, j] = -100\n",
    "        else:\n",
    "            # 如果找不到 ASSISTANT:，全部设为 -100\n",
    "            labels[i, :] = -100\n",
    "    \n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs\n",
    "\n",
    "print(\"✓ LLaVA collate 函数已定义\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49adabf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T20:10:59.401739600Z",
     "start_time": "2026-01-15T20:10:59.383735400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在重置 CUDA 状态...\n",
      "⚠️ 重置 CUDA 状态时出错: CUDA error: device-side assert triggered\n",
      "Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "建议：重启 Python 内核（Kernel -> Restart）\n"
     ]
    }
   ],
   "source": [
    "# ==== CUDA 状态重置工具（如果遇到 CUDA 超时错误，运行此 cell） ====\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"正在重置 CUDA 状态...\")\n",
    "    try:\n",
    "        # 清除所有 CUDA 缓存\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # 强制垃圾回收\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # 重置内存统计\n",
    "        try:\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        print(\"✓ CUDA 状态已重置\")\n",
    "        print(f\"当前 GPU 显存使用: {torch.cuda.memory_allocated() / 1024**3:.2f} GB / {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 重置 CUDA 状态时出错: {e}\")\n",
    "        print(\"建议：重启 Python 内核（Kernel -> Restart）\")\n",
    "else:\n",
    "    print(\"未检测到 CUDA，无需重置\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e79edd0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T20:11:01.466694900Z",
     "start_time": "2026-01-15T20:11:01.453226300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaVA-Med close 训练集大小: 954，steps/epoch≈954\n",
      "LLaVA-Med close 测试集大小: 239\n",
      "LLaVA-Med open 训练集大小: 844，steps/epoch≈844\n",
      "LLaVA-Med open 测试集大小: 211\n",
      "\n",
      "✓ DataLoader 已创建\n"
     ]
    }
   ],
   "source": [
    "# ==== 步骤5: 创建 DataLoader ====\n",
    "\n",
    "# LLaVA 模型较大，使用较小的 batch size（避免 CUDA 超时）\n",
    "# 如果仍然超时，可以进一步减小到 1\n",
    "llava_batch_size = 1  # 减小到 1 以避免 CUDA 超时\n",
    "\n",
    "# 创建 DataLoader\n",
    "llava_close_train_loader = DataLoader(\n",
    "    llava_close_train_dataset,\n",
    "    batch_size=llava_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=llava_collate_fn,\n",
    "    num_workers=0,  # Windows 上建议设为 0\n",
    ")\n",
    "\n",
    "llava_close_test_loader = DataLoader(\n",
    "    llava_close_test_dataset,\n",
    "    batch_size=llava_batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=llava_collate_fn,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "llava_open_train_loader = DataLoader(\n",
    "    llava_open_train_dataset,\n",
    "    batch_size=llava_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=llava_collate_fn,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "llava_open_test_loader = DataLoader(\n",
    "    llava_open_test_dataset,\n",
    "    batch_size=llava_batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=llava_collate_fn,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(f\"LLaVA-Med close 训练集大小: {len(llava_close_train_dataset)}，steps/epoch≈{len(llava_close_train_loader)}\")\n",
    "print(f\"LLaVA-Med close 测试集大小: {len(llava_close_test_dataset)}\")\n",
    "print(f\"LLaVA-Med open 训练集大小: {len(llava_open_train_dataset)}，steps/epoch≈{len(llava_open_train_loader)}\")\n",
    "print(f\"LLaVA-Med open 测试集大小: {len(llava_open_test_dataset)}\")\n",
    "print(\"\\n✓ DataLoader 已创建\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3042c54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T20:11:03.887995300Z",
     "start_time": "2026-01-15T20:11:03.826479600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ CUDA 初始化警告: CUDA error: device-side assert triggered\n",
      "Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "建议：如果持续出现 CUDA 超时，请重启 Python 内核\n",
      "✓ 已启用梯度检查点以节省显存\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/954 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "调试信息: vocab_size = 32000, image_token_id = 31999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(llava_close_train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m     53\u001b[39m     \u001b[38;5;66;03m# 在开始处理 batch 之前同步 CUDA\u001b[39;00m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available():\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m         \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     58\u001b[39m         \u001b[38;5;66;03m# 智能处理设备：获取模型各部分的设备\u001b[39;00m\n\u001b[32m     59\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\cuda\\__init__.py:1083\u001b[39m, in \u001b[36msynchronize\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m   1081\u001b[39m _lazy_init()\n\u001b[32m   1082\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.cuda.device(device):\n\u001b[32m-> \u001b[39m\u001b[32m1083\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_synchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# ==== 步骤6: 实现 LLaVA-Med Close 数据集训练循环 ====\n",
    "# \n",
    "# ⚠️ 重要提示：\n",
    "# 如果遇到 CUDA 超时错误，请：\n",
    "# 1. 重启 Python 内核（Kernel -> Restart）\n",
    "# 2. 重新运行所有前面的 cell\n",
    "# 3. 如果问题持续，考虑使用 4-bit 量化或减小 batch size\n",
    "\n",
    "# 设置 CUDA 超时时间（避免 CUDA 超时错误）\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # 同步模式，便于调试\n",
    "\n",
    "# 重置 CUDA 状态（如果 GPU 卡住了）\n",
    "try:\n",
    "    torch.cuda.set_device(0)\n",
    "    # 清除所有 CUDA 缓存\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    # 重置 CUDA 设备（如果可能）\n",
    "    try:\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    except:\n",
    "        pass\n",
    "    print(\"✓ CUDA 缓存已清除，设备已同步\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ CUDA 初始化警告: {e}\")\n",
    "    print(\"建议：如果持续出现 CUDA 超时，请重启 Python 内核\")\n",
    "\n",
    "# 启用梯度检查点以节省显存\n",
    "if hasattr(model, 'gradient_checkpointing_enable'):\n",
    "    model.gradient_checkpointing_enable()\n",
    "    print(\"✓ 已启用梯度检查点以节省显存\")\n",
    "\n",
    "# 设置训练参数\n",
    "num_epochs = 3\n",
    "lr = 2e-5\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# 训练循环\n",
    "model.train()\n",
    "best_loss = float('inf')\n",
    "\n",
    "# 错误计数器（如果连续错误太多，停止训练）\n",
    "consecutive_errors = 0\n",
    "max_consecutive_errors = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    successful_batches = 0\n",
    "    \n",
    "    for batch in tqdm(llava_close_train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # 在开始处理 batch 之前同步 CUDA\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        try:\n",
    "            # 智能处理设备：获取模型各部分的设备\n",
    "            try:\n",
    "                input_embed_layer = model.get_input_embeddings()\n",
    "                input_embed_device = next(input_embed_layer.parameters()).device\n",
    "            except:\n",
    "                input_embed_device = device\n",
    "            \n",
    "            try:\n",
    "                vision_device = next(model.vision_tower.parameters()).device\n",
    "            except:\n",
    "                vision_device = device\n",
    "            \n",
    "            # 同步后再移动 tensor\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            # 将输入移动到正确的设备（逐个移动，避免一次性移动太多）\n",
    "            new_batch = {}\n",
    "            for k, v in batch.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    if k in ['input_ids', 'attention_mask', 'labels']:\n",
    "                        new_batch[k] = v.to(input_embed_device, non_blocking=False)\n",
    "                    elif k == 'pixel_values':\n",
    "                        new_batch[k] = v.to(vision_device, non_blocking=False)\n",
    "                    else:\n",
    "                        new_batch[k] = v.to(device, non_blocking=False)\n",
    "                else:\n",
    "                    new_batch[k] = v\n",
    "            batch = new_batch\n",
    "            \n",
    "            # 移动完成后同步\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            # 前向传播（添加错误处理和 CUDA 同步）\n",
    "            # 添加 CUDA 同步点，确保之前的操作完成\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # 再次同步\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            # 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # 梯度裁剪\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # 同步确保优化器步骤完成\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            successful_batches += 1\n",
    "            consecutive_errors = 0  # 重置错误计数器\n",
    "            \n",
    "        except (RuntimeError, Exception) as e:\n",
    "            error_str = str(e)\n",
    "            consecutive_errors += 1\n",
    "            \n",
    "            if \"CUDA\" in error_str or \"timeout\" in error_str.lower() or \"AcceleratorError\" in error_str:\n",
    "                print(f\"\\n⚠️ CUDA 错误 ({consecutive_errors}/{max_consecutive_errors}): {error_str[:200]}...\")\n",
    "                print(\"尝试清除缓存并跳过此 batch...\")\n",
    "                try:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    torch.cuda.synchronize()\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # 如果连续错误太多，停止训练\n",
    "                if consecutive_errors >= max_consecutive_errors:\n",
    "                    print(f\"\\n❌ 连续 {max_consecutive_errors} 次 CUDA 错误，停止训练\")\n",
    "                    print(\"建议：请重启 Python 内核以清除 CUDA 状态，然后重新运行\")\n",
    "                    break\n",
    "                \n",
    "                # 跳过这个 batch，继续训练\n",
    "                continue\n",
    "            else:\n",
    "                # 其他错误，打印并继续\n",
    "                print(f\"\\n⚠️ 训练错误 ({consecutive_errors}/{max_consecutive_errors}): {error_str[:200]}...\")\n",
    "                try:\n",
    "                    torch.cuda.empty_cache()\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                if consecutive_errors >= max_consecutive_errors:\n",
    "                    print(f\"\\n❌ 连续 {max_consecutive_errors} 次错误，停止训练\")\n",
    "                    break\n",
    "                continue\n",
    "    \n",
    "    # 如果因为错误退出，跳出外层循环\n",
    "    if consecutive_errors >= max_consecutive_errors:\n",
    "        break\n",
    "    \n",
    "    avg_loss = epoch_loss / len(llava_close_train_loader)\n",
    "    print(f\"Epoch {epoch+1} / {num_epochs}, 平均 loss = {avg_loss:.4f}\")\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        save_path = \"best_llava_close.pth\"\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"✓ 保存最佳模型 (loss={avg_loss:.4f}) 到 {save_path}\")\n",
    "\n",
    "print(f\"\\n✓ LLaVA-Med close 微调完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88690e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 步骤7: 实现 LLaVA-Med Close 数据集评估函数 ====\n",
    "\n",
    "def is_semantically_similar(pred: str, gt: str) -> bool:\n",
    "    \"\"\"宽松的语义相似度判断（用于 close yes/no 问题）\"\"\"\n",
    "    pred = pred.lower().strip()\n",
    "    gt = gt.lower().strip()\n",
    "    \n",
    "    # 完全匹配\n",
    "    if pred == gt:\n",
    "        return True\n",
    "    \n",
    "    # 去除标点符号后比较\n",
    "    pred_clean = re.sub(r'[^\\w\\s]', '', pred)\n",
    "    gt_clean = re.sub(r'[^\\w\\s]', '', gt)\n",
    "    if pred_clean == gt_clean:\n",
    "        return True\n",
    "    \n",
    "    # 提取关键词\n",
    "    pred_words = set(re.findall(r'\\b\\w+\\b', pred))\n",
    "    gt_words = set(re.findall(r'\\b\\w+\\b', gt))\n",
    "    \n",
    "    # 移除停用词\n",
    "    stop_words = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'this', 'that'}\n",
    "    pred_words = pred_words - stop_words\n",
    "    gt_words = gt_words - stop_words\n",
    "    \n",
    "    # 计算重叠度\n",
    "    if len(gt_words) > 0:\n",
    "        overlap_ratio = len(pred_words & gt_words) / len(gt_words)\n",
    "        if overlap_ratio >= 0.5:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def eval_collate_fn(batch):\n",
    "    \"\"\"评估用的 collate 函数（不包含答案）\"\"\"\n",
    "    images, questions, answers = zip(*batch)\n",
    "    \n",
    "        # 获取图像 token ID（使用辅助函数确保有效性）\n",
    "    image_token_id = get_valid_image_token_id()\n",
    "    \n",
    "    # 计算每个图像需要多少个 token\n",
    "    patch_size = getattr(processor, 'patch_size', 14)\n",
    "    image_size = 336\n",
    "    num_image_tokens = (image_size // patch_size) ** 2  # 576\n",
    "    \n",
    "    # 构建 prompt（只包含问题，不包含答案）\n",
    "    prompts = [f\"USER: <image>\\n{q}\\nASSISTANT:\" for q in questions]\n",
    "    \n",
    "    # 处理图像\n",
    "    image_inputs = processor.image_processor(\n",
    "        list(images),\n",
    "        return_tensors=\"pt\",\n",
    "        size={\"height\": 336, \"width\": 336},\n",
    "        do_resize=True,\n",
    "        do_rescale=True,\n",
    "        do_normalize=True\n",
    "    )\n",
    "    \n",
    "    # 手动处理文本，插入图像 token\n",
    "    processed_input_ids = []\n",
    "    processed_attention_masks = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        image_pos = prompt.find('<image>')\n",
    "        if image_pos != -1:\n",
    "            before_image = prompt[:image_pos]\n",
    "            after_image = prompt[image_pos + len('<image>'):]\n",
    "            before_tokens = processor.tokenizer.encode(before_image, add_special_tokens=False)\n",
    "            after_tokens = processor.tokenizer.encode(after_image, add_special_tokens=False)\n",
    "            image_tokens = [image_token_id] * num_image_tokens\n",
    "            full_tokens = before_tokens + image_tokens + after_tokens\n",
    "        else:\n",
    "            full_tokens = processor.tokenizer.encode(prompt, add_special_tokens=False)\n",
    "        processed_input_ids.append(full_tokens)\n",
    "    \n",
    "    # Padding（限制最大长度避免 CUDA 超时）\n",
    "    max_length = 1024  # 减小最大长度避免超时\n",
    "    max_len = max(len(ids) for ids in processed_input_ids)\n",
    "    max_len = min(max_len, max_length)\n",
    "    \n",
    "    padded_input_ids = []\n",
    "    padded_attention_masks = []\n",
    "    \n",
    "    for ids in processed_input_ids:\n",
    "        if len(ids) > max_len:\n",
    "            ids = ids[:max_len]\n",
    "        attention_mask = [1] * len(ids) + [0] * (max_len - len(ids))\n",
    "        ids = ids + [processor.tokenizer.pad_token_id] * (max_len - len(ids))\n",
    "        padded_input_ids.append(ids)\n",
    "        padded_attention_masks.append(attention_mask)\n",
    "    \n",
    "    inputs = {\n",
    "        \"input_ids\": torch.tensor(padded_input_ids, dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor(padded_attention_masks, dtype=torch.long),\n",
    "        \"pixel_values\": image_inputs[\"pixel_values\"],\n",
    "    }\n",
    "    \n",
    "    return inputs, answers\n",
    "\n",
    "def evaluate_accuracy(model, data_loader, device):\n",
    "    \"\"\"评估准确率\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            inputs, true_answers = batch\n",
    "            \n",
    "            # 智能处理设备\n",
    "            try:\n",
    "                input_embed_layer = model.get_input_embeddings()\n",
    "                input_embed_device = next(input_embed_layer.parameters()).device\n",
    "            except:\n",
    "                input_embed_device = device\n",
    "            \n",
    "            try:\n",
    "                vision_device = next(model.vision_tower.parameters()).device\n",
    "            except:\n",
    "                vision_device = device\n",
    "            \n",
    "            # 移动到正确的设备\n",
    "            inputs = {\n",
    "                k: v.to(input_embed_device) if isinstance(v, torch.Tensor) and k in ['input_ids', 'attention_mask']\n",
    "                else (v.to(vision_device) if isinstance(v, torch.Tensor) and k == 'pixel_values'\n",
    "                else (v.to(device) if isinstance(v, torch.Tensor) else v))\n",
    "                for k, v in inputs.items()\n",
    "            }\n",
    "            \n",
    "            # 生成答案\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,  # yes/no 答案很短\n",
    "                do_sample=False,\n",
    "            )\n",
    "            \n",
    "            # 解码预测结果\n",
    "            preds = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "            \n",
    "            # 提取答案部分（ASSISTANT: 后面的内容）\n",
    "            preds_clean = []\n",
    "            for pred in preds:\n",
    "                if \"ASSISTANT:\" in pred:\n",
    "                    pred_answer = pred.split(\"ASSISTANT:\")[-1].strip()\n",
    "                else:\n",
    "                    pred_answer = pred.strip()\n",
    "                preds_clean.append(pred_answer)\n",
    "            \n",
    "            # 计算准确率\n",
    "            for pred_raw, true_ans_raw in zip(preds_clean, true_answers):\n",
    "                pred = normalize_answer(pred_raw)\n",
    "                true_ans = normalize_answer(true_ans_raw)\n",
    "                \n",
    "                if is_semantically_similar(pred, true_ans):\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy, correct, total\n",
    "\n",
    "# 创建评估用的 DataLoader\n",
    "llava_close_test_loader_eval = DataLoader(\n",
    "    llava_close_test_dataset,\n",
    "    batch_size=llava_batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=eval_collate_fn,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "# 评估 close 数据集\n",
    "print(\"评估 close 测试集...\")\n",
    "accuracy, correct, total = evaluate_accuracy(model, llava_close_test_loader_eval, device)\n",
    "print(f\"\\nClose 测试集准确率: {accuracy:.4f} ({correct}/{total})\")\n",
    "print(\"✓ 评估完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a4005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 步骤8: 实现 LLaVA-Med Open 数据集训练循环 ====\n",
    "\n",
    "# 重新加载一个干净的模型用于 open 数据微调（避免受 close 微调的影响）\n",
    "print(\"重新加载干净的 LLaVA-Med 模型用于 open 数据微调...\")\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "model_open_loaded = False\n",
    "\n",
    "# 策略1：先尝试不使用量化\n",
    "if not use_quantization and torch.cuda.is_available():\n",
    "    try:\n",
    "        model_open = LlavaForConditionalGeneration.from_pretrained(\n",
    "            model_local_path,\n",
    "            dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        print(\"✓ 模型已加载（无量化）\")\n",
    "        model_open_loaded = True\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower() or \"OOM\" in str(e):\n",
    "            print(f\"⚠️ 显存不足，切换到4-bit量化...\")\n",
    "            use_quantization = True\n",
    "\n",
    "# 策略2：使用4-bit量化\n",
    "if not model_open_loaded and use_quantization and torch.cuda.is_available():\n",
    "    try:\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "        model_open = LlavaForConditionalGeneration.from_pretrained(\n",
    "            model_local_path,\n",
    "            dtype=torch.float16,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        print(\"✓ 模型已加载（4-bit 量化）\")\n",
    "        model_open_loaded = True\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 量化加载失败: {e}\")\n",
    "        raise e\n",
    "\n",
    "# CPU模式\n",
    "if not model_open_loaded:\n",
    "    model_open = LlavaForConditionalGeneration.from_pretrained(\n",
    "        model_local_path,\n",
    "        dtype=torch.float32,\n",
    "        device_map=None,\n",
    "    )\n",
    "    model_open = model_open.to(device)\n",
    "    print(\"✓ 模型已加载（CPU模式）\")\n",
    "\n",
    "# 启用梯度检查点\n",
    "if hasattr(model_open, 'gradient_checkpointing_enable'):\n",
    "    model_open.gradient_checkpointing_enable()\n",
    "\n",
    "# 训练参数\n",
    "optimizer_open = AdamW(model_open.parameters(), lr=lr)\n",
    "\n",
    "# 训练循环\n",
    "model_open.train()\n",
    "best_loss_open = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_open.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for batch in tqdm(llava_open_train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # 智能处理设备\n",
    "        try:\n",
    "            input_embed_layer = model_open.get_input_embeddings()\n",
    "            input_embed_device = next(input_embed_layer.parameters()).device\n",
    "        except:\n",
    "            input_embed_device = device\n",
    "        \n",
    "        try:\n",
    "            vision_device = next(model_open.vision_tower.parameters()).device\n",
    "        except:\n",
    "            vision_device = device\n",
    "        \n",
    "        # 移动到正确的设备\n",
    "        batch = {\n",
    "            k: v.to(input_embed_device) if isinstance(v, torch.Tensor) and k in ['input_ids', 'attention_mask', 'labels']\n",
    "            else (v.to(vision_device) if isinstance(v, torch.Tensor) and k == 'pixel_values'\n",
    "            else (v.to(device) if isinstance(v, torch.Tensor) else v))\n",
    "            for k, v in batch.items()\n",
    "        }\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model_open(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # 反向传播\n",
    "        optimizer_open.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_open.parameters(), max_norm=1.0)\n",
    "        optimizer_open.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(llava_open_train_loader)\n",
    "    print(f\"Epoch {epoch+1} / {num_epochs}, 平均 loss = {avg_loss:.4f}\")\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if avg_loss < best_loss_open:\n",
    "        best_loss_open = avg_loss\n",
    "        save_path_open = \"best_llava_open.pth\"\n",
    "        torch.save(model_open.state_dict(), save_path_open)\n",
    "        print(f\"✓ 保存最佳模型 (loss={avg_loss:.4f}) 到 {save_path_open}\")\n",
    "\n",
    "print(f\"\\n✓ LLaVA-Med open 微调完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7aef31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 步骤9: 实现 LLaVA-Med Open 数据集评估函数 ====\n",
    "\n",
    "def is_semantically_similar_open(pred: str, gt: str) -> bool:\n",
    "    \"\"\"宽松的语义相似度判断（用于 open 问题）\"\"\"\n",
    "    pred = pred.lower().strip()\n",
    "    gt = gt.lower().strip()\n",
    "    \n",
    "    # 完全匹配\n",
    "    if pred == gt:\n",
    "        return True\n",
    "    \n",
    "    # 去除标点符号后比较\n",
    "    pred_clean = re.sub(r'[^\\w\\s]', '', pred)\n",
    "    gt_clean = re.sub(r'[^\\w\\s]', '', gt)\n",
    "    if pred_clean == gt_clean:\n",
    "        return True\n",
    "    \n",
    "    # 一方包含另一方\n",
    "    if gt in pred or pred in gt:\n",
    "        return True\n",
    "    \n",
    "    # 提取关键词\n",
    "    pred_words = set(re.findall(r'\\b\\w+\\b', pred))\n",
    "    gt_words = set(re.findall(r'\\b\\w+\\b', gt))\n",
    "    \n",
    "    # 移除停用词\n",
    "    stop_words = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'this', 'that', 'of', 'in', 'on', 'at', 'to', 'for'}\n",
    "    pred_words = pred_words - stop_words\n",
    "    gt_words = gt_words - stop_words\n",
    "    \n",
    "    # 计算重叠度\n",
    "    if len(gt_words) > 0:\n",
    "        overlap_ratio = len(pred_words & gt_words) / len(gt_words)\n",
    "        if overlap_ratio >= 0.5:\n",
    "            return True\n",
    "    \n",
    "    # 处理同义词\n",
    "    synonyms = {\n",
    "        'x-ray': ['xray', 'x ray', 'chest xray', 'chest x-ray'],\n",
    "        'xray': ['x-ray', 'x ray'],\n",
    "        'ct': ['ct scan', 'computed tomography'],\n",
    "        'mri': ['magnetic resonance imaging'],\n",
    "        'left': ['lt', 'left side'],\n",
    "        'right': ['rt', 'right side'],\n",
    "    }\n",
    "    \n",
    "    for key, variants in synonyms.items():\n",
    "        if key in gt and any(v in pred for v in variants):\n",
    "            return True\n",
    "        if key in pred and any(v in gt for v in variants):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def eval_open_collate_fn(batch):\n",
    "    \"\"\"Open 数据集评估用的 collate 函数\"\"\"\n",
    "    images, questions, answers = zip(*batch)\n",
    "    \n",
    "        # 获取图像 token ID（使用辅助函数确保有效性）\n",
    "    image_token_id = get_valid_image_token_id()\n",
    "    \n",
    "    # 计算每个图像需要多少个 token\n",
    "    patch_size = getattr(processor, 'patch_size', 14)\n",
    "    image_size = 336\n",
    "    num_image_tokens = (image_size // patch_size) ** 2  # 576\n",
    "    \n",
    "    prompts = [f\"USER: <image>\\n{q}\\nASSISTANT:\" for q in questions]\n",
    "    \n",
    "    # 处理图像\n",
    "    image_inputs = processor.image_processor(\n",
    "        list(images),\n",
    "        return_tensors=\"pt\",\n",
    "        size={\"height\": 336, \"width\": 336},\n",
    "        do_resize=True,\n",
    "        do_rescale=True,\n",
    "        do_normalize=True\n",
    "    )\n",
    "    \n",
    "    # 手动处理文本，插入图像 token\n",
    "    processed_input_ids = []\n",
    "    processed_attention_masks = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        image_pos = prompt.find('<image>')\n",
    "        if image_pos != -1:\n",
    "            before_image = prompt[:image_pos]\n",
    "            after_image = prompt[image_pos + len('<image>'):]\n",
    "            before_tokens = processor.tokenizer.encode(before_image, add_special_tokens=False)\n",
    "            after_tokens = processor.tokenizer.encode(after_image, add_special_tokens=False)\n",
    "            image_tokens = [image_token_id] * num_image_tokens\n",
    "            full_tokens = before_tokens + image_tokens + after_tokens\n",
    "        else:\n",
    "            full_tokens = processor.tokenizer.encode(prompt, add_special_tokens=False)\n",
    "        processed_input_ids.append(full_tokens)\n",
    "    \n",
    "    # Padding（限制最大长度避免 CUDA 超时）\n",
    "    max_length = 1024  # 减小最大长度避免超时\n",
    "    max_len = max(len(ids) for ids in processed_input_ids)\n",
    "    max_len = min(max_len, max_length)\n",
    "    \n",
    "    padded_input_ids = []\n",
    "    padded_attention_masks = []\n",
    "    \n",
    "    for ids in processed_input_ids:\n",
    "        if len(ids) > max_len:\n",
    "            ids = ids[:max_len]\n",
    "        attention_mask = [1] * len(ids) + [0] * (max_len - len(ids))\n",
    "        ids = ids + [processor.tokenizer.pad_token_id] * (max_len - len(ids))\n",
    "        padded_input_ids.append(ids)\n",
    "        padded_attention_masks.append(attention_mask)\n",
    "    \n",
    "    inputs = {\n",
    "        \"input_ids\": torch.tensor(padded_input_ids, dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor(padded_attention_masks, dtype=torch.long),\n",
    "        \"pixel_values\": image_inputs[\"pixel_values\"],\n",
    "    }\n",
    "    \n",
    "    return inputs, answers\n",
    "\n",
    "def evaluate_accuracy_open(model, data_loader, device):\n",
    "    \"\"\"评估 open 数据集的准确率\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            inputs, true_answers = batch\n",
    "            \n",
    "            # 智能处理设备\n",
    "            try:\n",
    "                input_embed_layer = model.get_input_embeddings()\n",
    "                input_embed_device = next(input_embed_layer.parameters()).device\n",
    "            except:\n",
    "                input_embed_device = device\n",
    "            \n",
    "            try:\n",
    "                vision_device = next(model.vision_tower.parameters()).device\n",
    "            except:\n",
    "                vision_device = device\n",
    "            \n",
    "            # 移动到正确的设备\n",
    "            inputs = {\n",
    "                k: v.to(input_embed_device) if isinstance(v, torch.Tensor) and k in ['input_ids', 'attention_mask']\n",
    "                else (v.to(vision_device) if isinstance(v, torch.Tensor) and k == 'pixel_values'\n",
    "                else (v.to(device) if isinstance(v, torch.Tensor) else v))\n",
    "                for k, v in inputs.items()\n",
    "            }\n",
    "            \n",
    "            # 生成答案（open 问题答案可能较长）\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=30,\n",
    "                do_sample=False,\n",
    "            )\n",
    "            \n",
    "            # 解码\n",
    "            preds = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "            \n",
    "            # 提取答案部分\n",
    "            preds_clean = []\n",
    "            for pred in preds:\n",
    "                if \"ASSISTANT:\" in pred:\n",
    "                    pred_answer = pred.split(\"ASSISTANT:\")[-1].strip()\n",
    "                else:\n",
    "                    pred_answer = pred.strip()\n",
    "                preds_clean.append(pred_answer)\n",
    "            \n",
    "            # 计算准确率\n",
    "            for pred_raw, true_ans_raw in zip(preds_clean, true_answers):\n",
    "                pred = normalize_answer(pred_raw)\n",
    "                true_ans = normalize_answer(true_ans_raw)\n",
    "                \n",
    "                if is_semantically_similar_open(pred, true_ans):\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy, correct, total\n",
    "\n",
    "# 创建 open 测试集的 DataLoader（使用评估用的 collate 函数）\n",
    "llava_open_test_loader_eval = DataLoader(\n",
    "    llava_open_test_dataset,\n",
    "    batch_size=llava_batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=eval_open_collate_fn,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "# 评估 open 数据集\n",
    "print(\"评估 open 测试集...\")\n",
    "accuracy_open, correct_open, total_open = evaluate_accuracy_open(model_open, llava_open_test_loader_eval, device)\n",
    "print(f\"\\nOpen 测试集准确率: {accuracy_open:.4f} ({correct_open}/{total_open})\")\n",
    "print(\"✓ 评估完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaeb85aae8f484b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
